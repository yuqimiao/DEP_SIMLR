---
title: "choose_eigenvec_number"
author: "yuqimiao"
date: "2022-10-06"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

# Introduction

This notebook investigate the number of eigenvectors to use in DEP-SIMLR

```{r, include = F}
library(tidyverse)
library(igraph)
source("code/functions/Partition_CIMLR_2.0.R")
source("code/functions/CIMLR_kernel_input.R")
source("code/functions/visualization_functions.R")
get_data_4clust = function(n_sub = 200,
                           n_feat = 1000,
                           s2n_ratio = 0.05, # signal to noise ratio
                           mu_vec = c(1, 2, 10, 20), # feature mean for 4 clusters
                           signal_sd = 1,
                           noise_sd = 1){
  data = matrix(0, n_sub, n_feat)
  n_signal = floor(n_feat*s2n_ratio)
  signal_ind = 1:n_signal
  noise_ind = (n_signal+1):n_feat


  n_sub_perc = n_sub/4
  cluster_index = lapply(1:4, function(i) ((i-1)*n_sub_perc+1):(i*n_sub_perc))

  # noise simulation
  data[1:n_sub, noise_ind]=rnorm(n = n_sub*length(noise_ind), mean = 0, sd = noise_sd)


  # signal_simulation
  for(i in 1:4){
    cluster_index_cur = cluster_index[[i]]
    mu_cur = mu_vec[i]
    data[cluster_index_cur, signal_ind]=rnorm(n = n_sub_perc*length(signal_ind), mean = mu_cur, sd = signal_sd)
  }

  return(data)
}
```

# DEP-SIMLR optimization review 

DEP-SIMLR

**Input:** 

* $Z_0$: Initialized similarity list for each data type, 
* $c$: number of clusters for integrated data

**Output: ** 

* $Z$: List of updated similarity for each data type
* $F$: List of partition information for each data type
* $Y$: Partition information for integrated data type

**Initialize**

1. $Z_0$, $w_0$
2. Choose number of vectors to use in $F_0$ and initialize $F_0$
3. $Y_0 = \bf 0$

**Repeat**, iteration t,

1. $F_{t-1} \rightarrow Z_t$
2. $Z_t, Y_{t-1} \rightarrow F_t$
3. $F_t \rightarrow Y_t$
4. $F_t, Y_t, \rightarrow w_t$


# When to estimate number of eigen-vector to use?

Among the iteration steps, there are 2 steps used the eigenvectors to represents the graph laplacian matrix, Details can be found in Overleaf. Here we explore the performance of eigengap, argue that this should be used as a criteria to choose the number of eigenvector to use instead f=of an estimation of number of clusters in the graph when the graph is not clear

data simulation, small noise
```{r}
## simulate
noise_sd = 1 # small noise
n_feat1 = 1000
n_feat2 = 100000
mu1 = c(0, 0, 1, -1)
mu2 = c(1,-1,0,0)

data1 = get_data_4clust(n_feat = n_feat1, mu_vec = mu1, noise_sd = noise_sd)
data2 = get_data_4clust(n_feat = n_feat2, mu_vec = mu2, noise_sd = noise_sd)

data_list = list(data1, data2)
## benchmark
alpha = 1
sigma = 2 # kernel_parameter
diffusion_form = "L1"
n = nrow(data_list[[1]])
k = floor(sqrt(n))
# dist, kernel
distance_list = lapply(data_list, function(x) dist2(x,x))
kernel_list = lapply(distance_list, function(x) kernel_calculation(distance = x, k = k, sigma = sigma ))

```

single kernel eigenspace visual and cluster
```{r}
# Show clear structure
S1 = kernel_list[[1]]
S2 = kernel_list[[2]]
diag(S1) = mean(S1)
diag(S2) = mean(S2)
heatmap_gg(S1,"data kernel")
heatmap_gg(S2,"data2, kernel")
# heatmap_gg(S1+S2,"kernel sum")

heatmap_gg(eigen(S1)$vectors[,1:5],"S1 1-5 largest eigenvec")
heatmap_gg(eigen(S2)$vectors[,1:5],"S2 1-5 largest eigenvec")

est_nclust(S1)
est_nclust(S2)

L1 = normalized_GL(S1)
plot(2:10, sort(eigen(L1)$values)[2:10], main = "GL 1 1-5 smallest eigenval")

L2 = normalized_GL(S2)
plot(2:10, sort(eigen(L2)$values)[2:10], main = "GL 2 1-5 smallest eigenval")

heatmap_gg(eigen(L1)$vectors[,(n-5):n],"GL 1-5 smallest eigenvec")
heatmap_gg(eigen(L2)$vectors[,(n-5):n],"GL 1-5 smallest eigenvec")

c_single = 3
compare(kmeans(eigen(L1)$vectors[,(n-c_single+1):n],3, nstart = 200)$cluster, c(rep(1,100), rep(2:3, each = 50)), "nmi")
compare(kmeans(eigen(L2)$vectors[,(n-c_single+1):n],3, nstart = 200)$cluster, c(rep(1:2, each = 50), rep(3,100)), "nmi")

c_single = 2
compare(kmeans(eigen(L1)$vectors[,(n-c_single+1):n],3, nstart = 200)$cluster, c(rep(1,100), rep(2:3, each = 50)), "nmi")
compare(kmeans(eigen(L2)$vectors[,(n-c_single+1):n],3, nstart = 200)$cluster, c(rep(1:2, each = 50), rep(3,100)), "nmi")

```

We can see clearly, we should use the first 2 eigenvector to for 3 clusters in both data 1 and data 2

For integrated Y learn from FsFs^T, see overleaf for how we learn Y
```{r}
# learned Y, no weight
c_single = c(2,2)

Fs = list(data1 = eigen(L1)$vectors[,(n-c_single[1]+1):n], data2 = eigen(L2)$vectors[,(n-c_single[2]+1):n])
Ls = lapply(Fs, function(f) diag(1,n)-f %*% t(f) *2)
Ly = Reduce("+", Ls)

c = 4
Y = eigen(Ly)$vectors[, (n-c+1):n]
plot(2:10, sort(eigen(Ly)$values)[2:10], main = "eigvec from learned Y")
heatmap_gg(Y, "learned Y, no weight")
compare(kmeans(Y, 4, nstart = 200)$cluster, rep(1:4, each = 50), "nmi")

c = 3
Y = eigen(Ly)$vectors[, (n-c+1):n]
compare(kmeans(Y, 4, nstart = 200)$cluster, rep(1:4, each = 50), "nmi")
```

Thus in our algorithm we estimate numbe of clusters in both step 2 and 3

# Discussion next:
How to estimate the number of vectors to use for Fs?

Fixing Z and Y, the update of Fs is

$$
F_s = arg\min_{F_s} tr(F_s^TM(Z_s,Y)F_s) \\
M(Z_s,Y) = \gamma(L_{Z_s}+\rho L_{YY^T})
$$

Explore: 

* How will sum of 2 matrices influence the eigenspace?
* Should we use $Z_s$ or $M(Z_s, Y)$ to estimate the number of eigenvectors used in the single data partition information?



